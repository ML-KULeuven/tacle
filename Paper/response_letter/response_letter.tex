\documentclass{article}
\usepackage{enumerate}
\usepackage[english]{babel}
\usepackage{times}
\usepackage{hyperref}
\usepackage{latexsym}    % to get LASY symbols
\usepackage{graphicx}    % to insert PostScript figures
\usepackage{xspace}
\usepackage[usenames,dvipsnames]{xcolor}


\newcommand{\sergey}[1]{\textcolor{magenta}{{\sc Sergey:} #1}\xspace}
\newcommand{\sam}[1]{\textcolor{green}{{\sc Samuel:} #1}\xspace}
\newcommand{\samuel}[1]{\textcolor{green}{{\sc Samuel:} #1}\xspace}
\newcommand{\tias}[1]{\textcolor{blue}{{\sc Tias:} #1}\xspace}
\newcommand{\luc}[1]{\textcolor{red}{{\sc Luc:} #1}\xspace}
\newcommand{\added}[1]{\textcolor{red}{added:} {#1} \textcolor{red}{end}\xspace}

\newcommand{\scaleconstant}{1.0}
\bibliographystyle{plainnat}
\author{Samuel Kolb \and Sergey Paramonov \and Tias Guns \and Luc {De Raedt}}
\title{MLJ Revision Letter:\\ learning constraints in spreadsheets and tabular data}
\begin{document}
\maketitle
\sergey{We need a discussion section on: noise, (non)-contiguous blocks and combination of constraints} 
\sergey{just in case, the text is preliminary, feel free to cut, expand and rewrite, I think it should be 1 or 2 pages at the end, I hope}
\section{Key issues}

\paragraph{Goal and motivation}
\sergey{Reviewer pointed out that (and we should thank him and use it, I think):}

\textbf{Reviewer \#3:}
Section 6. The reviewer would like to point out the important field of data cleaning (of dirty data, as it is called in the database community) and anomaly detection, beyond simple error detection. Because your method does not only detect errors, but can also show *why* it deems something as erronous your method falls in the desirable category of descriptive or explainable anomaly detection algorithms.\\
\sergey{A sentence on compression, maybe?}\\
\sergey{Can we claim as a contribution creation of the first dataset for tabular constraint learning? I guess it is a valid claim, it didn't exist before at all, right?}\\
\sergey{We need to connect better Sections 5 and 6 -- from the reviews}

\paragraph{Approach}
\sergey{here is the response on the unclear definitions and notation}

\paragraph{Competitor algorithms and related work:} \textit{The reviewers point out that you should compare with other methods. One reviewer stresses that Microsoft Excel provides functionality that alleviates part of the problem you address. Another reviewer suggests that you compare to a baseline that considers all constraints. You must compare to algorithms that solve the same problem (at least part of it).}

\sergey{Tias, you also had a point here that flash fill is searching for a combination of string transformations, we search for atomic constraints and etc, could you integrate it here? and indicate what part should be added to the paper?}

We have elaborated the differences between TaCLe and FlashFill in the related work section (highlighted in \textcolor{red}{red}), also indicating why FlashFill cannot be applied to the examples we consider. Let us summarize the key differences here \sergey{Samuel, you had a point on this, can you add it ?}:
\begin{itemize}
  \item FlashFill works only with a single table, while TaCLe can handle multiple tables
  \item FlashFill only searches for a string transformation program, while TaCLe works with textual and numeric constraints
  \item FlashFill requires marked input-output columns (and columns are treated as vectors), while TaCLe is unsupervised and works across columns and rows
  \item \sergey{here should be the point by Samuel} like they produce a single program with a single solution, while we enumerate multiple constraints that hold in the data
\end{itemize}


Also, we have compared TaCLe with a baseline: one where the graph dependency is disabled and another where we disabled Step 1 (candidate generation based on properties) and graph dependency. \sergey{Samuel, Tias -- can you expand and correct this one?}

\section{Detailed comments}

\paragraph{Sensitivity to the noise}
In the field of Inductive Logic Programming and Constraint Learning, there has been a line of work devoted to noise handling. Let us briefly overview the key methods applicable in our case:
\begin{itemize}
  \item \textbf{Noisy semantics}: a possible approach is to modify the semantics of constraints such as $\sum_{i=1}^n b_i = c$ into $|\sum_{i=1}^n b_i - c| \leq \epsilon$, where $\epsilon$ is the noise level that might be dependent on $b_i$'s, $n$ or $c$.
  \item \textbf{Skip semantics}: another possibility is to allow skipping of elements in vectors, i.e. in the constraint  $\sum_{i=1}^n b_i = c$, we can allow $i$ to skip up to $k$ elements to match $c$, then we treat skipped values as noise and do not take them into account.
  \item \textbf{Soft constraints}: a constraint satisfaction is determined by a real-value measure such as $|\sum_{i=1}^n b_i - c | = \sigma|c|$, where $\sigma=0$ denotes the perfect match and the other values denote deviations in absolute values of $c$, then our goal would be to select top-$k$ best matching constraints according to the designed measure.
\end{itemize}

We have expanded the conclusions section to accommodate these proposals.


\paragraph{How the data was obtained and its detailed overview}
We have extended the description of the gathered collection of spreadsheets in the paper (Section 5.3), also we provided an extensive technical description of the dataset in the corresponding github repository: \url{https://github.com/sergeyparamonov/tacle} and added a table indicating present constraints by category into the appendix.
\sergey{Samuel, can you make a detailed overview of the dataset in github https://github.com/sergeyparamonov/tacle}?

\sergey{Also, they wanted some real examples of formulas from the dataset in Section 6}


\sergey{Samuel? I guess, we have to say that it should be specified in the constraint itself, on what it depends, kind of given}~ \textbf{Reviwer \#2}: C. According to my understanding, the proposed method exploits possible dependencies and redundancies that may exist between the different constraints. However, it is unclear how such dependencies/redundancies are found. Are they assumed to be given as input? Are they detected by the algorithm? What would be the performance (runtime and precision/recall) of the algorithm if no dependencies/redundancies exist?



\end{document}
