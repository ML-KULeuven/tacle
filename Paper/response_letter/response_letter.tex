\documentclass{article}
\usepackage[usenames,dvipsnames,table]{xcolor}
\usepackage{enumerate}
\usepackage[english]{babel}
\usepackage{times}
\usepackage{hyperref}
\usepackage{latexsym}    % to get LASY symbols
\usepackage{graphicx}    % to insert PostScript figures
\usepackage{xspace}
\usepackage[usenames,dvipsnames]{xcolor}


\newcommand{\sergey}[1]{\textcolor{magenta}{{\sc Sergey:} #1}\xspace}
\newcommand{\sam}[1]{\textcolor{green}{{\sc Samuel:} #1}\xspace}
\newcommand{\samuel}[1]{\textcolor{green}{{\sc Samuel:} #1}\xspace}
\newcommand{\tias}[1]{\textcolor{blue}{{\sc Tias:} #1}\xspace}
\newcommand{\luc}[1]{\textcolor{red}{{\sc Luc:} #1}\xspace}
\newcommand{\added}[1]{\textcolor{red}{added:} {#1} \textcolor{red}{end}\xspace}

\newcommand{\scaleconstant}{1.0}
\bibliographystyle{plainnat}
\author{Samuel Kolb \and Sergey Paramonov \and Tias Guns \and Luc {De Raedt}}
\title{MLJ Revision Letter:\\ learning constraints in spreadsheets and tabular data}
\begin{document}
\maketitle
\sergey{We need a discussion section on: noise, (non)-contiguous blocks and combination of constraints} 
\sergey{just in case, the text is preliminary, feel free to cut, expand and rewrite, I think it should be 1 or 2 pages at the end, I hope}
\section{Key issues}

\paragraph{Goal and motivation}
\sergey{Reviewer pointed out that (and we should thank him and use it, I think):}

\textbf{Reviewer \#3:}
Section 6. The reviewer would like to point out the important field of data cleaning (of dirty data, as it is called in the database community) and anomaly detection, beyond simple error detection. Because your method does not only detect errors, but can also show *why* it deems something as erronous your method falls in the desirable category of descriptive or explainable anomaly detection algorithms.\\
\sergey{A sentence on compression, maybe?}\\
\sergey{Can we claim as a contribution creation of the first dataset for tabular constraint learning? I guess it is a valid claim, it didn't exist before at all, right?}\\
\sergey{We need to connect better Sections 5 and 6 -- from the reviews}

\paragraph{Approach}
\sergey{here is the response on the unclear definitions and notation}

\paragraph{Competitor algorithms and related work:} \textit{The reviewers point out that you should compare with other methods. One reviewer stresses that Microsoft Excel provides functionality that alleviates part of the problem you address. Another reviewer suggests that you compare to a baseline that considers all constraints. You must compare to algorithms that solve the same problem (at least part of it).}

\sergey{Tias, you also had a point here that flash fill is searching for a combination of string transformations, we search for atomic constraints and etc, could you integrate it here? and indicate what part should be added to the paper?}

We have elaborated the differences between TaCLe and FlashFill in the related work section (highlighted in \textcolor{red}{red}), also indicating why FlashFill cannot be applied to the examples we consider. Let us summarize the key differences here \sergey{Samuel, you had a point on this, can you add it ?}:
\begin{itemize}
  \item FlashFill works only with a single table, while TaCLe can handle multiple tables
  \item FlashFill only searches for a string transformation program, while TaCLe works with textual and numeric constraints
  \item FlashFill requires marked input-output columns (and columns are treated as vectors), while TaCLe is unsupervised and works across columns and rows
  \item \sergey{here should be the point by Samuel} like they produce a single program with a single solution, while we enumerate multiple constraints that hold in the data
\end{itemize}


Also, we have compared TaCLe with a baseline: one where the graph dependency is disabled and another where we disabled Step 1 (candidate generation based on properties) and graph dependency. \sergey{Samuel, Tias -- can you expand and correct this one?}

\section{Detailed comments}


\paragraph{Reviewer \#2, point B:How the data was obtained and its detailed overview}
We have extended the description of the gathered collection of spreadsheets in the paper (Section 5.3), also we provided an extensive technical description of the dataset in the corresponding github repository: \url{https://github.com/SergeyParamonov/tacle} and added a table indicating present constraints by category into the Appendix together with description of spreadsheet sources. Let us present the overview here as well: there are three main sources for spreadsheets we have collected.
\begin{itemize}
  \item After identification of popular Excel functions (the MS Office web page has an overview of popular functions), we have searched for online tutorials about these functions and collected them under the category \textbf{Tutorials} (a link to each webpage found and used is provided in the accompanying github repository).
  \item We have collected the exercises under the category \textbf{Exercises} from the introductory Excel book \cite{excel_book} that focused on popular Excel functions.
  \item We have collected under the category \textbf{Data} publicly available economic and crime reporting data from the U.S. Bureau of Economic Analysis (BEA) and U.S. FBI Uniform Crime Reporting (UCR) Program (the links are also provided in the accompanying github repository).
\end{itemize}

The overview of constraint distributions per category is presented in Table \ref{tab:constraint_by_category}.
\begin{table}
  \centering
  \rowcolors{2}{gray!25}{white}
  \caption{Constraint occurrence by category}
  \label{tab:constraint_by_category}
  \begin{tabular}{l | c c c }
    \textbf{Constraint} & \textbf{Exercises} & \textbf{Data} & \textbf{Tutorials}  \\\hline
    average (col) & 0 & 0 & 2 \\
    average (row) & 0 & 1 & 0 \\
    average-if    & 1 & 0 & 0 \\
    count (col) & 0 & 0 & 1 \\
    count-if & 1 & 0 & 0 \\
    difference & 2 & 1 & 0 \\
    equal & 0 & 0 & 10 \\
    foreign-product & 1 & 0 & 0 \\
    fuzzy-lookup & 2 & 0 & 1 \\
    lookup & 1 & 0 & 0 \\
    max (col) & 0 & 0 & 2 \\
    max-if & 1 & 0 & 0 \\
    min (col) & 0 & 0 & 3 \\
    min-if & 1 & 0 & 0 \\
    percentual-diff & 5 & 1 & 0 \\
    product & 2 & 0 & 3 \\
    project & 1 & 0 & 0 \\
    rank & 1 & 0 & 0 \\
    series & 2 & 1 & 0 \\
    sum (col) & 5 & 0 & 10 \\
    sum (row) & 2 & 2 & 3 \\
    sum-if & 2 & 0 & 9 \\
    sum-product & 0 & 0 & 2 \\
  \end{tabular}
\end{table}

\sergey{Samuel, can you have a look at what I wrote in the appendix and in the README and expand it?} 

\sergey{Also, they wanted some real examples of formulas from the dataset in Section 6}


\textbf{Reviwer \#2, point C}:  \textit{According to my understanding, the proposed method exploits possible dependencies and redundancies that may exist between the different constraints. However, it is unclear how such dependencies/redundancies are found. Are they assumed to be given as input? Are they detected by the algorithm? What would be the performance (runtime and precision/recall) of the algorithm if no dependencies/redundancies exist?}

\sergey{Samuel? I guess, we have to say that it should be specified in the constraint itself, on what it depends, kind of given}~


Also, we have evaluated the influence on the runtime of the presence of the dependent graph by disabling it in the system and comparing the results on the datasets and synthetic data as presented in \sergey{Samuel, could you put a ref to where the baseline experiment is?}. There is no difference in the precision/recall, since the dependency graph only allows to avoid checking constraints that are guaranteed to fail, i.e., without the dependency graph the system would have to check the constraints that will fail to establish that indeed they do not hold.


\paragraph{Review \#3, major drawback of the method: sensitivity to the noise}
In the field of Inductive Logic Programming and Constraint Learning, there has been a line of work devoted to noise handling. Let us briefly overview the key methods applicable in our case:
\begin{itemize}
  \item \textbf{Noisy semantics}: a possible approach is to modify the semantics of constraints such as $\sum_{i=1}^n b_i = c$ into $|\sum_{i=1}^n b_i - c| \leq \epsilon$, where $\epsilon$ is the noise level that might be dependent on $b_i$'s, $n$ or $c$.
  \item \textbf{Skip semantics}: another possibility is to allow skipping of elements in vectors, i.e. in the constraint  $\sum_{i=1}^n b_i = c$, we can allow $i$ to skip up to $k$ elements to match $c$, then we treat skipped values as noise and do not take them into account.
  \item \textbf{Soft constraints}: a constraint satisfaction is determined by a real-value measure such as $|\sum_{i=1}^n b_i - c | = \sigma|c|$, where $\sigma=0$ denotes the perfect match and the other values denote deviations in absolute values of $c$, then our goal would be to select top-$k$ best matching constraints according to the designed measure.
\end{itemize}

We have expanded the conclusions section to accommodate these proposals.

\paragraph{Reviewer \#3, (non)-contiguous blocks} \textit{
The requirement that blocks are contiguous makes sense, but also feels like a severe limitation. Please explain more clearly why this is (not) the case.}
\sergey{Samuel, we have a short discussion that it breaks everything, right? Like you cannot prune, for example and also that skip semantics kind of imitates non-contiguous blocks}


\bibliography{references}
\end{document}
